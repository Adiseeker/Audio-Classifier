{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9532e826-0187-46a7-9cd8-47216ec12e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|                                                | 0/1 [00:00<?, ?model/s]\n",
      "Model mistral-small | Chunk 128:   0%|                                  | 0/97 [00:00<?, ?file/s]\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (2888 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Model mistral-small | Chunk 128:   1%|▏                       | 1/97 [00:46<1:14:05, 46.31s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:   2%|▍                       | 2/97 [01:27<1:08:17, 43.13s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:   3%|▋                       | 3/97 [02:14<1:10:29, 44.99s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:   4%|▉                       | 4/97 [02:59<1:09:53, 45.10s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:   5%|█▏                      | 5/97 [03:54<1:14:37, 48.67s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:   6%|█▍                      | 6/97 [04:47<1:16:06, 50.18s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:   7%|█▋                      | 7/97 [05:45<1:18:52, 52.59s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:   8%|█▉                      | 8/97 [06:46<1:22:03, 55.32s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:   9%|██▏                     | 9/97 [07:40<1:20:18, 54.75s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  10%|██▎                    | 10/97 [08:36<1:20:00, 55.17s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  11%|██▌                    | 11/97 [09:36<1:21:23, 56.78s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  12%|██▊                    | 12/97 [10:37<1:22:07, 57.96s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  13%|███                    | 13/97 [11:35<1:21:06, 57.93s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  14%|███▎                   | 14/97 [12:29<1:18:28, 56.73s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  15%|███▌                   | 15/97 [13:25<1:17:20, 56.59s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  16%|███▊                   | 16/97 [14:23<1:16:57, 57.00s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  18%|████                   | 17/97 [15:09<1:11:47, 53.84s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  19%|████▎                  | 18/97 [15:59<1:09:06, 52.49s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  20%|████▌                  | 19/97 [16:50<1:07:46, 52.14s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  21%|████▋                  | 20/97 [17:51<1:10:28, 54.91s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  22%|████▉                  | 21/97 [19:03<1:15:47, 59.84s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  23%|█████▏                 | 22/97 [20:05<1:15:49, 60.66s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  24%|█████▍                 | 23/97 [21:05<1:14:26, 60.35s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  25%|█████▋                 | 24/97 [21:55<1:09:37, 57.23s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  26%|█████▉                 | 25/97 [22:47<1:06:53, 55.75s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  27%|██████▏                | 26/97 [23:52<1:09:17, 58.55s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  28%|██████▍                | 27/97 [24:48<1:07:27, 57.82s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  29%|██████▋                | 28/97 [25:39<1:04:08, 55.78s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  30%|██████▉                | 29/97 [26:37<1:03:56, 56.42s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  31%|███████                | 30/97 [27:34<1:03:12, 56.61s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  32%|███████▎               | 31/97 [28:42<1:05:58, 59.98s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  33%|███████▌               | 32/97 [29:47<1:06:26, 61.34s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  34%|███████▊               | 33/97 [30:46<1:04:57, 60.89s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  35%|████████               | 34/97 [31:51<1:05:00, 61.92s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  36%|████████▎              | 35/97 [33:02<1:06:46, 64.63s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  37%|█████████▎               | 36/97 [33:45<59:20, 58.37s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  38%|█████████▌               | 37/97 [34:48<59:37, 59.62s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  39%|█████████▊               | 38/97 [35:41<56:35, 57.56s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  40%|██████████               | 39/97 [36:41<56:29, 58.43s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  41%|█████████▍             | 40/97 [37:58<1:00:39, 63.85s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  42%|█████████▋             | 41/97 [39:04<1:00:21, 64.67s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  43%|██████████▊              | 42/97 [40:11<59:56, 65.39s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  44%|██████████▏            | 43/97 [41:34<1:03:21, 70.40s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  45%|██████████▍            | 44/97 [42:46<1:02:44, 71.03s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  46%|███████████▌             | 45/97 [43:39<56:54, 65.66s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  47%|███████████▊             | 46/97 [44:44<55:39, 65.48s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  48%|████████████             | 47/97 [45:47<53:59, 64.79s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  49%|████████████▎            | 48/97 [46:42<50:18, 61.59s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  51%|████████████▋            | 49/97 [47:13<42:08, 52.68s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  52%|████████████▉            | 50/97 [48:25<45:38, 58.26s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  53%|█████████████▏           | 51/97 [49:34<47:13, 61.60s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  54%|█████████████▍           | 52/97 [49:52<36:29, 48.65s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  55%|█████████████▋           | 53/97 [50:15<29:53, 40.75s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  56%|█████████████▉           | 54/97 [50:31<23:55, 33.38s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  57%|██████████████▏          | 55/97 [50:51<20:38, 29.49s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  58%|██████████████▍          | 56/97 [51:11<18:05, 26.47s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - dĹ‚ugi 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 128:  60%|██████████████▉          | 58/97 [51:33<12:36, 19.39s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  61%|███████████████▏         | 59/97 [51:53<12:17, 19.40s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  62%|███████████████▍         | 60/97 [52:08<11:22, 18.44s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  63%|███████████████▋         | 61/97 [52:22<10:13, 17.03s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  64%|███████████████▉         | 62/97 [52:40<10:11, 17.46s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  65%|████████████████▏        | 63/97 [52:55<09:29, 16.74s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  66%|████████████████▍        | 64/97 [53:25<11:19, 20.59s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  67%|████████████████▊        | 65/97 [53:42<10:27, 19.60s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  68%|█████████████████        | 66/97 [53:57<09:22, 18.14s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  69%|█████████████████▎       | 67/97 [54:15<08:59, 17.99s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  70%|█████████████████▌       | 68/97 [54:30<08:17, 17.14s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  71%|█████████████████▊       | 69/97 [54:54<08:57, 19.19s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  72%|██████████████████       | 70/97 [55:15<08:51, 19.68s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  73%|██████████████████▎      | 71/97 [55:34<08:26, 19.49s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - los klasy Ĺ›redniej 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 128:  75%|██████████████████▊      | 73/97 [55:48<05:29, 13.74s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - mieszkania dla mĹ‚odych 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 128:  77%|███████████████████▎     | 75/97 [56:08<04:32, 12.36s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  78%|███████████████████▌     | 76/97 [56:25<04:42, 13.44s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  79%|███████████████████▊     | 77/97 [56:42<04:44, 14.23s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  80%|████████████████████     | 78/97 [56:59<04:44, 14.99s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  81%|████████████████████▎    | 79/97 [57:18<04:49, 16.07s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  82%|████████████████████▌    | 80/97 [57:33<04:25, 15.64s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  84%|████████████████████▉    | 81/97 [57:52<04:28, 16.76s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  85%|█████████████████████▏   | 82/97 [58:10<04:15, 17.00s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  86%|█████████████████████▍   | 83/97 [58:25<03:51, 16.55s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  87%|█████████████████████▋   | 84/97 [58:49<04:02, 18.63s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  88%|█████████████████████▉   | 85/97 [59:07<03:39, 18.32s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  89%|██████████████████████▏  | 86/97 [59:23<03:16, 17.87s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - upadek bankĂłw 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 128:  91%|██████████████████████▋  | 88/97 [59:45<02:11, 14.62s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  92%|█████████████████████  | 89/97 [1:00:05<02:06, 15.85s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  93%|█████████████████████▎ | 90/97 [1:00:16<01:42, 14.70s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  94%|█████████████████████▌ | 91/97 [1:00:32<01:30, 15.14s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - wÄ™giel cz1.txt\n",
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - wÄ™giel po zimie 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 128:  97%|██████████████████████▎| 94/97 [1:00:59<00:35, 11.75s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128:  98%|██████████████████████▌| 95/97 [1:01:16<00:25, 12.75s/file]\u001b[A\n",
      "Model mistral-small | Chunk 128: 100%|███████████████████████| 97/97 [1:01:54<00:00, 38.29s/file]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - Ĺ‚apĂłwki 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 512:   0%|                                  | 0/97 [00:00<?, ?file/s]\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (2888 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Model mistral-small | Chunk 512:   1%|▎                         | 1/97 [00:25<40:06, 25.06s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:   2%|▌                         | 2/97 [00:45<35:05, 22.16s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:   3%|▊                         | 3/97 [01:08<35:51, 22.89s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:   4%|█                         | 4/97 [01:31<35:16, 22.76s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:   5%|█▎                        | 5/97 [01:59<37:54, 24.72s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:   6%|█▌                        | 6/97 [02:25<37:49, 24.94s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:   7%|█▉                        | 7/97 [02:53<39:06, 26.07s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:   8%|██▏                       | 8/97 [03:20<39:03, 26.33s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:   9%|██▍                       | 9/97 [03:48<39:20, 26.82s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  10%|██▌                      | 10/97 [04:14<38:47, 26.75s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  11%|██▊                      | 11/97 [04:46<40:17, 28.11s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  12%|███                      | 12/97 [05:16<40:50, 28.83s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  13%|███▎                     | 13/97 [05:48<41:30, 29.64s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  14%|███▌                     | 14/97 [06:14<39:42, 28.70s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  15%|███▊                     | 15/97 [06:43<39:07, 28.63s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  16%|████                     | 16/97 [07:08<37:18, 27.64s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  18%|████▍                    | 17/97 [07:32<35:24, 26.56s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  19%|████▋                    | 18/97 [07:56<34:07, 25.92s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  20%|████▉                    | 19/97 [08:21<33:22, 25.68s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  21%|█████▏                   | 20/97 [08:53<35:03, 27.31s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  22%|█████▍                   | 21/97 [09:31<38:46, 30.61s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  23%|█████▋                   | 22/97 [10:00<37:46, 30.22s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  24%|█████▉                   | 23/97 [10:29<36:38, 29.71s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  25%|██████▏                  | 24/97 [10:55<34:43, 28.54s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  26%|██████▍                  | 25/97 [11:24<34:26, 28.70s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  27%|██████▋                  | 26/97 [11:58<36:04, 30.48s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  28%|██████▉                  | 27/97 [12:25<34:06, 29.24s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  29%|███████▏                 | 28/97 [12:49<31:50, 27.68s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  30%|███████▍                 | 29/97 [13:19<32:16, 28.47s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  31%|███████▋                 | 30/97 [13:48<32:03, 28.71s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  32%|███████▉                 | 31/97 [14:21<32:46, 29.79s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  33%|████████▏                | 32/97 [14:52<32:46, 30.25s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  34%|████████▌                | 33/97 [15:21<31:55, 29.93s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  35%|████████▊                | 34/97 [15:51<31:29, 30.00s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  36%|█████████                | 35/97 [16:27<32:44, 31.68s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  37%|█████████▎               | 36/97 [16:49<29:15, 28.78s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  38%|█████████▌               | 37/97 [17:21<29:44, 29.74s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  39%|█████████▊               | 38/97 [17:44<27:19, 27.79s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  40%|██████████               | 39/97 [18:16<27:56, 28.90s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  41%|██████████▎              | 40/97 [18:53<29:59, 31.57s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  42%|██████████▌              | 41/97 [19:26<29:50, 31.98s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  43%|██████████▊              | 42/97 [19:59<29:36, 32.30s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  44%|███████████              | 43/97 [20:39<30:59, 34.44s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  45%|███████████▎             | 44/97 [21:16<31:11, 35.31s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  46%|███████████▌             | 45/97 [21:42<28:05, 32.41s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  47%|███████████▊             | 46/97 [22:13<27:20, 32.17s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  48%|████████████             | 47/97 [22:44<26:23, 31.67s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  49%|████████████▎            | 48/97 [23:10<24:31, 30.03s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  51%|████████████▋            | 49/97 [23:27<20:50, 26.06s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  52%|████████████▉            | 50/97 [24:03<22:41, 28.97s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  53%|█████████████▏           | 51/97 [24:36<23:20, 30.45s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  54%|█████████████▍           | 52/97 [24:46<18:06, 24.14s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  55%|█████████████▋           | 53/97 [24:56<14:37, 19.94s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  56%|█████████████▉           | 54/97 [25:03<11:35, 16.17s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  57%|██████████████▏          | 55/97 [25:13<10:01, 14.33s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  58%|██████████████▍          | 56/97 [25:23<08:49, 12.91s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - dĹ‚ugi 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 512:  60%|██████████████▉          | 58/97 [25:33<05:58,  9.20s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  61%|███████████████▏         | 59/97 [25:43<06:03,  9.57s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  62%|███████████████▍         | 60/97 [25:51<05:33,  9.01s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  63%|███████████████▋         | 61/97 [25:57<04:59,  8.32s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  64%|███████████████▉         | 62/97 [26:07<05:08,  8.81s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  65%|████████████████▏        | 63/97 [26:16<04:54,  8.66s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  66%|████████████████▍        | 64/97 [26:30<05:40, 10.30s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  67%|████████████████▊        | 65/97 [26:38<05:03,  9.49s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  68%|█████████████████        | 66/97 [26:45<04:38,  9.00s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  69%|█████████████████▎       | 67/97 [26:55<04:32,  9.07s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  70%|█████████████████▌       | 68/97 [27:02<04:11,  8.67s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  71%|█████████████████▊       | 69/97 [27:15<04:34,  9.80s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  72%|██████████████████       | 70/97 [27:25<04:26,  9.88s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  73%|██████████████████▎      | 71/97 [27:34<04:12,  9.72s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - los klasy Ĺ›redniej 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 512:  75%|██████████████████▊      | 73/97 [27:41<02:43,  6.81s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - mieszkania dla mĹ‚odych 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 512:  77%|███████████████████▎     | 75/97 [27:51<02:14,  6.11s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  78%|███████████████████▌     | 76/97 [28:00<02:22,  6.79s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  79%|███████████████████▊     | 77/97 [28:08<02:21,  7.05s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  80%|████████████████████     | 78/97 [28:16<02:18,  7.31s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  81%|████████████████████▎    | 79/97 [28:25<02:18,  7.72s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  82%|████████████████████▌    | 80/97 [28:32<02:07,  7.48s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  84%|████████████████████▉    | 81/97 [28:42<02:09,  8.07s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  85%|█████████████████████▏   | 82/97 [28:51<02:08,  8.59s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  86%|█████████████████████▍   | 83/97 [29:01<02:02,  8.78s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  87%|█████████████████████▋   | 84/97 [29:12<02:02,  9.46s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  88%|█████████████████████▉   | 85/97 [29:21<01:52,  9.39s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  89%|██████████████████████▏  | 86/97 [29:32<01:47,  9.82s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - upadek bankĂłw 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 512:  91%|██████████████████████▋  | 88/97 [29:43<01:11,  7.90s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  92%|██████████████████████▉  | 89/97 [29:53<01:07,  8.48s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  93%|███████████████████████▏ | 90/97 [30:00<00:56,  8.06s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  94%|███████████████████████▍ | 91/97 [30:09<00:49,  8.18s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - wÄ™giel cz1.txt\n",
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - wÄ™giel po zimie 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 512:  97%|████████████████████████▏| 94/97 [30:23<00:19,  6.39s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512:  98%|████████████████████████▍| 95/97 [30:32<00:13,  6.83s/file]\u001b[A\n",
      "Model mistral-small | Chunk 512: 100%|█████████████████████████| 97/97 [30:49<00:00, 19.07s/file]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - Ĺ‚apĂłwki 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 1024:   0%|                                 | 0/97 [00:00<?, ?file/s]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   1%|▏                      | 1/97 [01:14<1:59:23, 74.62s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   2%|▍                      | 2/97 [02:19<1:49:09, 68.94s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   3%|▋                      | 3/97 [03:42<1:57:53, 75.25s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   4%|▉                      | 4/97 [04:46<1:49:37, 70.73s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   5%|█▏                     | 5/97 [06:05<1:52:58, 73.68s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   6%|█▍                     | 6/97 [07:09<1:46:56, 70.51s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   7%|█▋                     | 7/97 [08:39<1:55:14, 76.83s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   8%|█▉                     | 8/97 [09:57<1:54:26, 77.15s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:   9%|██▏                    | 9/97 [11:18<1:55:16, 78.60s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  10%|██▎                   | 10/97 [12:26<1:48:50, 75.07s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  11%|██▍                   | 11/97 [13:40<1:47:19, 74.88s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  12%|██▋                   | 12/97 [14:53<1:45:19, 74.34s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  13%|██▉                   | 13/97 [16:29<1:53:13, 80.88s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  14%|███▏                  | 14/97 [17:45<1:49:48, 79.38s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  15%|███▍                  | 15/97 [18:55<1:44:46, 76.67s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  16%|███▋                  | 16/97 [20:02<1:39:35, 73.78s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  18%|███▊                  | 17/97 [21:03<1:32:57, 69.71s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  19%|████                  | 18/97 [22:03<1:28:11, 66.98s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  20%|████▎                 | 19/97 [23:16<1:29:21, 68.74s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  21%|████▌                 | 20/97 [24:43<1:35:13, 74.21s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  22%|████▊                 | 21/97 [26:15<1:40:55, 79.68s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  23%|████▉                 | 22/97 [27:59<1:48:23, 86.72s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  24%|█████▏                | 23/97 [29:18<1:44:14, 84.52s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  25%|█████▍                | 24/97 [30:32<1:39:04, 81.43s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  26%|█████▋                | 25/97 [31:47<1:35:24, 79.51s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  27%|█████▉                | 26/97 [33:09<1:35:00, 80.28s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  28%|██████                | 27/97 [34:28<1:33:10, 79.87s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  29%|██████▎               | 28/97 [35:37<1:27:58, 76.50s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  30%|██████▌               | 29/97 [36:44<1:23:39, 73.81s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  31%|██████▊               | 30/97 [38:13<1:27:13, 78.11s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  32%|███████               | 31/97 [39:39<1:28:31, 80.48s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  33%|███████▎              | 32/97 [40:54<1:25:25, 78.85s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  34%|███████▍              | 33/97 [42:20<1:26:25, 81.02s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  35%|███████▋              | 34/97 [43:48<1:27:20, 83.19s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  36%|███████▉              | 35/97 [45:08<1:25:01, 82.27s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  37%|████████▏             | 36/97 [46:02<1:15:05, 73.85s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  38%|████████▍             | 37/97 [47:17<1:14:13, 74.23s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  39%|████████▌             | 38/97 [48:12<1:07:10, 68.31s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  40%|████████▊             | 39/97 [49:26<1:07:42, 70.04s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  41%|█████████             | 40/97 [50:57<1:12:25, 76.23s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  42%|█████████▎            | 41/97 [52:26<1:14:50, 80.18s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  43%|█████████▌            | 42/97 [53:51<1:14:45, 81.55s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  44%|█████████▊            | 43/97 [55:42<1:21:30, 90.56s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  45%|█████████▉            | 44/97 [57:02<1:17:05, 87.26s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  46%|██████████▏           | 45/97 [58:24<1:14:12, 85.62s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  47%|██████████▍           | 46/97 [59:47<1:12:09, 84.90s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  48%|█████████▋          | 47/97 [1:01:05<1:09:00, 82.80s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  49%|█████████▉          | 48/97 [1:02:12<1:03:50, 78.17s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  51%|███████████           | 49/97 [1:03:02<55:49, 69.79s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  52%|██████████▎         | 50/97 [1:04:46<1:02:29, 79.78s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  53%|██████████▌         | 51/97 [1:06:15<1:03:29, 82.82s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  54%|███████████▊          | 52/97 [1:06:48<50:46, 67.69s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  55%|████████████          | 53/97 [1:07:24<42:40, 58.19s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  56%|████████████▏         | 54/97 [1:07:53<35:26, 49.46s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  57%|████████████▍         | 55/97 [1:08:27<31:20, 44.77s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  58%|████████████▋         | 56/97 [1:08:59<28:05, 41.12s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - dĹ‚ugi 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 1024:  60%|█████████████▏        | 58/97 [1:09:34<19:34, 30.13s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  61%|█████████████▍        | 59/97 [1:10:08<19:45, 31.18s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  62%|█████████████▌        | 60/97 [1:10:32<18:01, 29.22s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  63%|█████████████▊        | 61/97 [1:10:51<15:52, 26.46s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  64%|██████████████        | 62/97 [1:11:26<16:48, 28.82s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  65%|██████████████▎       | 63/97 [1:11:52<15:52, 28.01s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  66%|██████████████▌       | 64/97 [1:12:36<17:53, 32.53s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  67%|██████████████▋       | 65/97 [1:13:01<16:16, 30.51s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  68%|██████████████▉       | 66/97 [1:13:26<14:55, 28.88s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  69%|███████████████▏      | 67/97 [1:14:03<15:32, 31.09s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  70%|███████████████▍      | 68/97 [1:14:29<14:22, 29.75s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  71%|███████████████▋      | 69/97 [1:15:06<14:48, 31.72s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  72%|███████████████▉      | 70/97 [1:15:35<14:01, 31.16s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  73%|████████████████      | 71/97 [1:16:14<14:29, 33.45s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - los klasy Ĺ›redniej 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 1024:  75%|████████████████▌     | 73/97 [1:16:35<09:09, 22.89s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - mieszkania dla mĹ‚odych 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 1024:  77%|█████████████████     | 75/97 [1:17:07<07:23, 20.17s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  78%|█████████████████▏    | 76/97 [1:17:35<07:38, 21.84s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  79%|█████████████████▍    | 77/97 [1:17:57<07:19, 21.95s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  80%|█████████████████▋    | 78/97 [1:18:34<08:09, 25.77s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  81%|█████████████████▉    | 79/97 [1:19:01<07:47, 25.99s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  82%|██████████████████▏   | 80/97 [1:19:17<06:34, 23.21s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  84%|██████████████████▎   | 81/97 [1:20:00<07:39, 28.74s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  85%|██████████████████▌   | 82/97 [1:20:34<07:36, 30.45s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  86%|██████████████████▊   | 83/97 [1:21:01<06:52, 29.43s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  87%|███████████████████   | 84/97 [1:21:38<06:49, 31.52s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  88%|███████████████████▎  | 85/97 [1:22:08<06:12, 31.02s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  89%|███████████████████▌  | 86/97 [1:22:47<06:08, 33.49s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - upadek bankĂłw 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 1024:  91%|███████████████████▉  | 88/97 [1:23:30<04:11, 27.98s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  92%|████████████████████▏ | 89/97 [1:24:09<04:05, 30.63s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  93%|████████████████████▍ | 90/97 [1:24:35<03:25, 29.41s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  94%|████████████████████▋ | 91/97 [1:25:11<03:08, 31.43s/file]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - wÄ™giel cz1.txt\n",
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - wÄ™giel po zimie 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model mistral-small | Chunk 1024:  97%|█████████████████████▎| 94/97 [1:26:05<01:12, 24.13s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024:  98%|█████████████████████▌| 95/97 [1:26:38<00:51, 25.92s/file]\u001b[A\n",
      "Model mistral-small | Chunk 1024: 100%|██████████████████████| 97/97 [1:27:34<00:00, 54.17s/file]\u001b[A\n",
      "Processing models: 100%|███████████████████████████████████| 1/1 [3:00:20<00:00, 10820.60s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - Ĺ‚apĂłwki 1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import ollama\n",
    "\n",
    "# Function to select the best tokenizer based on the chunk size\n",
    "def select_best_tokenizer(chunk_size):\n",
    "    if chunk_size == 128:\n",
    "        return AutoTokenizer.from_pretrained(\"bert-base-uncased\"), AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "    elif chunk_size == 512:\n",
    "        return AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\"), AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "    else:\n",
    "        # Using Longformer for larger chunks (1024)\n",
    "        return AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\"), AutoModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "def split_text_into_chunks(text, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Split text into chunks of max_length tokens, recursively splitting chunks \n",
    "    that are still too long.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='np', truncation=False, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "\n",
    "    # If the text fits within the max_length, return it as a single chunk\n",
    "    if len(input_ids) <= max_length:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(input_ids), max_length):\n",
    "        chunk_ids = input_ids[i:i + max_length]\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Check if the chunk is still too long\n",
    "        if len(tokenizer(chunk_text)['input_ids']) > max_length:\n",
    "            # Recursively split the chunk\n",
    "            sub_chunks = split_text_into_chunks(chunk_text, tokenizer, max_length)\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def compute_embeddings(texts, tokenizer, model, max_length):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            embedding = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(embedding)\n",
    "    return torch.mean(torch.stack(embeddings), dim=0)\n",
    "\n",
    "def tag_text(text, model_name):\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are a professional tagger. Your task is to analyze a given text and return highly relevant tags to the main topics and themes of the text. \n",
    "\n",
    "            Guidelines:\n",
    "            1. Only provide the tags, nothing else.\n",
    "            2. Each tag must be a single word, not a phrase.\n",
    "            3. Separate the tags with commas, without spaces or additional formatting.\n",
    "\n",
    "            Example Input:\n",
    "            \"Artificial intelligence is transforming industries like healthcare, finance, and transportation.\"\n",
    "\n",
    "            Example Output:\n",
    "            ai,technology,automation\n",
    "\n",
    "            Now, generate tags for the following text:\n",
    "            \"{text}\"\n",
    "            \"\"\"\n",
    "        }]\n",
    "    )\n",
    "    if isinstance(response, dict):\n",
    "        tags = response.get('message', {}).get('content', \"error,generating,tags\").split(',')\n",
    "    else:\n",
    "        tags = [\"error\", \"generating\", \"tags\"]\n",
    "    return [tag.strip() for tag in tags]\n",
    "\n",
    "def save_tags_to_csv(output_path, tags):\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['tag_name', 'cosine_similarity'])\n",
    "        for tag, similarity in tags:\n",
    "            csv_writer.writerow([tag, f\"{similarity:.4f}\"])\n",
    "\n",
    "def process_file(file_path, model_name, output_folder, tokenizer, model, chunk_size, iterations):\n",
    "    \"\"\"Process the file, generate tags, and save the best tags.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        original_text = f.read()\n",
    "\n",
    "    # Split the text into chunks\n",
    "    chunks = split_text_into_chunks(original_text, tokenizer, max_length=chunk_size)\n",
    "\n",
    "    # Compute the embeddings of the entire text (average embedding across all chunks)\n",
    "    text_embedding = compute_embeddings(chunks, tokenizer, model, max_length=chunk_size)\n",
    "\n",
    "    chunk_folder = os.path.join(output_folder, f\"chunk_{chunk_size}\")\n",
    "    os.makedirs(chunk_folder, exist_ok=True)\n",
    "\n",
    "    all_tags_per_iteration = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        tags = []\n",
    "        tag_similarities = {}\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_tags = tag_text(chunk, model_name)\n",
    "            tags.extend(chunk_tags)\n",
    "\n",
    "        # Remove duplicates and calculate cosine similarity\n",
    "        for tag in set(tags):\n",
    "            tag_embedding = compute_embeddings([tag], tokenizer, model, max_length=chunk_size)\n",
    "            similarity = cosine_similarity(\n",
    "                text_embedding.cpu().numpy(),\n",
    "                tag_embedding.cpu().numpy()\n",
    "            )[0][0]  # Scalar similarity value\n",
    "\n",
    "            if tag not in tag_similarities or similarity > tag_similarities[tag]:\n",
    "                tag_similarities[tag] = similarity\n",
    "\n",
    "        # Save the tags for this iteration\n",
    "        sorted_tags = sorted(tag_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        all_tags_per_iteration.append(sorted_tags)\n",
    "\n",
    "        output_file = os.path.join(\n",
    "            chunk_folder,\n",
    "            f\"{os.path.splitext(os.path.basename(file_path))[0]}_{i+1}.csv\"\n",
    "        )\n",
    "        save_tags_to_csv(output_file, sorted_tags)\n",
    "\n",
    "    # Save the \"best tags\" file as a copy of the first iteration\n",
    "    best_tags_file = os.path.join(\n",
    "        chunk_folder,\n",
    "        f\"{os.path.splitext(os.path.basename(file_path))[0]}_best_tags.csv\"\n",
    "    )\n",
    "    os.rename(output_file, best_tags_file)\n",
    "\n",
    "def initialize_environment(chunk_size):\n",
    "    tokenizer, model = select_best_tokenizer(chunk_size)\n",
    "    return tokenizer, model\n",
    "\n",
    "def read_file_paths(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        return [line.strip().replace('.mp3', '.txt') for line in f]\n",
    "\n",
    "def create_output_folder(model_name):\n",
    "    safe_model_name = model_name.replace(\":\", \"_\").replace(\"<\", \"_\").replace(\">\", \"_\")\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    folder_name = f\"{safe_model_name}_{timestamp}\"\n",
    "    output_folder = os.path.join(\"voiceapp\", \"output\", \"tagging_folder\", folder_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    return output_folder\n",
    "\n",
    "def main():\n",
    "    input_file = 'D:\\\\Ai\\\\Audio-Classifier\\\\voiceapp\\\\lista-1.txt'\n",
    "    models_list = [        \n",
    "        # 'mistral:7b',\n",
    "        'mistral-small',\n",
    "        # 'llama3.2:3b',\n",
    "        # 'qwen2:7b',\n",
    "        # 'yi',\n",
    "        #'glm4:9b',\n",
    "        # 'qwen2.5:7b',\n",
    "        # 'qwen2.5:72b'\n",
    "    ]\n",
    "\n",
    "    file_paths = read_file_paths(input_file)\n",
    "    for model_name in tqdm(models_list, desc=\"Processing models\", unit=\"model\"):\n",
    "        output_folder = create_output_folder(model_name)\n",
    "        for chunk_size in [128, 512, 1024]:\n",
    "            tokenizer, model = initialize_environment(chunk_size)\n",
    "            for file_path in tqdm(file_paths, desc=f\"Model {model_name} | Chunk {chunk_size}\", unit=\"file\"):\n",
    "                if os.path.exists(file_path):\n",
    "                    process_file(file_path, model_name, output_folder, tokenizer, model, chunk_size, iterations=5)\n",
    "                else:\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b46de81-bdf2-4c57-809b-7268ebccb59f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m     generate_summary_csv(summary_csv_path, summary_data)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAi\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAudio-Classifier\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvoiceapp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mlista-1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m models_list \u001b[38;5;241m=\u001b[39m [        \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral:7b\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral-small\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqwen2.5:72b\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[1;32m---> 14\u001b[0m output_folders, tokenizer, model \u001b[38;5;241m=\u001b[39m initialize_environment(models_list)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Read file paths\u001b[39;00m\n\u001b[0;32m     17\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m read_file_paths(input_file)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize variables\n",
    "    input_file = 'D:\\\\Ai\\\\Audio-Classifier\\\\voiceapp\\\\lista-1.txt'\n",
    "    models_list = [        \n",
    "        # 'mistral:7b',\n",
    "        # 'mistral-small',\n",
    "        #  'llama3.2:3b',\n",
    "        # 'qwen2:7b',\n",
    "        # 'yi',\n",
    "        'glm4:9b',\n",
    "        # 'qwen2.5:7b',\n",
    "        # 'qwen2.5:72b'\n",
    "    ]\n",
    "    output_folders, tokenizer, model = initialize_environment(models_list)\n",
    "\n",
    "    # Read file paths\n",
    "    file_paths = read_file_paths(input_file)\n",
    "\n",
    "    # Initialize summary tracking\n",
    "    summary_data = []\n",
    "\n",
    "    # Process each model sequentially\n",
    "    for model_name in models_list:\n",
    "        print(f\"\\nProcessing all files for model: {model_name}\")\n",
    "        pbar = tqdm(total=len(file_paths), desc=f\"Processing {model_name}\", unit=\"file\")\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            output_folder = output_folders[model_name]\n",
    "            best_tags_file = process_file(file_path, model_name, output_folder, tokenizer, model, iterations=5)\n",
    "            summary_data.append({\n",
    "                \"Original File Path\": file_path,\n",
    "                \"Model\": model_name,\n",
    "                \"Tags File Path\": best_tags_file\n",
    "            })\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    # Generate summary CSV\n",
    "    summary_csv_path = 'output_summary.csv'\n",
    "    generate_summary_csv(summary_csv_path, summary_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5b6a32f-149f-4e94-8a45-93ad469a3dae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     generate_summary_csv(output_folder, input_files, tag_files)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m reference_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Optional reference file for metrics\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize tokenizer and model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tokenizer, model \u001b[38;5;241m=\u001b[39m initialize_environment([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-distilroberta-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Process each file\u001b[39;00m\n\u001b[0;32m      8\u001b[0m tag_files \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    reference_file = \"reference.txt\"  # Optional reference file for metrics\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer, model = initialize_environment([\"sentence-transformers/all-distilroberta-v1\"])\n",
    "    \n",
    "    # Process each file\n",
    "    tag_files = []\n",
    "    for file_path in input_files:\n",
    "        best_tags_file, _ = process_file_with_metrics(file_path, \"model_name\", output_folder, tokenizer, model, reference_file=reference_file)\n",
    "        tag_files.append(best_tags_file)\n",
    "    \n",
    "    # Generate summary CSV\n",
    "    generate_summary_csv(output_folder, input_files, tag_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcba4b-111c-4d49-a94b-c4d2ea004b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
