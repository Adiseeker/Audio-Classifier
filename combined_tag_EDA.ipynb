{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39a0abb2-c82a-4014-be34-f88b261dd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import ollama\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Load a pre-trained model for semantic similarity\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "\n",
    "# Function to compute embeddings\n",
    "def compute_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Function to compute cosine similarity between two embeddings\n",
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Function to split text into chunks of approximately 100 words\n",
    "def split_into_chunks(text, chunk_size=100):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Function to chunk the tags list to avoid exceeding the token limit\n",
    "def chunk_tags(tag_list, max_tags_per_chunk=100):\n",
    "    return [tag_list[i:i + max_tags_per_chunk] for i in range(0, len(tag_list), max_tags_per_chunk)]\n",
    "\n",
    "# Helper to get the current timestamp\n",
    "def get_timestamp():\n",
    "    return time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Function to create a folder based on the model name and timestamp\n",
    "def create_output_folder(model_name):\n",
    "    \"\"\"\n",
    "    Creates a timestamped output folder for a given model name.\n",
    "    Replaces forbidden characters in the folder name.\n",
    "    \"\"\"\n",
    "    # Create the base output and tagging folders\n",
    "    base_output_path = os.path.join(\"voiceapp\", \"output\")\n",
    "    combined_tags_folder_path = os.path.join(base_output_path, \"combined_tags_folder\")\n",
    "    \n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    os.makedirs(combined_tags_folder_path, exist_ok=True)\n",
    "\n",
    "    # Sanitize the model name by replacing forbidden characters with '_'\n",
    "    safe_model_name = re.sub(r'[<>:\"/\\\\|?*]', '_', model_name)\n",
    "\n",
    "    # Generate a timestamp and create the final folder name\n",
    "    timestamp = get_timestamp()\n",
    "    folder_name = f\"{safe_model_name}_{timestamp}\"\n",
    "\n",
    "    # Create the model-specific folder\n",
    "    model_folder_path = os.path.join(combined_tags_folder_path, folder_name)\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    return model_folder_path\n",
    "\n",
    "# Step 1: Combine all tags into combine_tags.csv\n",
    "def combine_tags_from_files(lista_path, output_folder):\n",
    "    combined_tags = set()\n",
    "    with open(lista_path, 'r') as f:\n",
    "        file_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    total_files = len(file_paths)\n",
    "    pbar = tqdm(total=total_files, desc=\"Combining tags from files\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        file_path = file_path.replace('.mp3', '_best_tags.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    if 'tag_name' in row:\n",
    "                        combined_tags.add(row['tag_name'].strip())\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # Write to combine_tags.csv\n",
    "    output_path = os.path.join(output_folder, 'combine_tags.csv')\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['tag_name'])\n",
    "        for tag in sorted(combined_tags):  # Sort for consistency\n",
    "            writer.writerow([tag])\n",
    "    return output_path\n",
    "\n",
    "# Step 2: Clean tags\n",
    "def clean_tags(input_path, output_folder):\n",
    "    def is_similar(word1, word2, threshold=0.8):\n",
    "        return SequenceMatcher(None, word1, word2).ratio() > threshold\n",
    "\n",
    "    cleaned_tags = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        tags = [row['tag_name'].strip() for row in reader if 'tag_name' in row]\n",
    "\n",
    "    # Remove duplicates and similar tags\n",
    "    for tag in tags:\n",
    "        cleaned_tag = tag.strip()\n",
    "        if cleaned_tag not in cleaned_tags and len(cleaned_tag) <= 50 and re.match(\"^[a-zA-Z0-9_-]+$\", cleaned_tag):\n",
    "            if all(not is_similar(cleaned_tag, existing_tag) for existing_tag in cleaned_tags):\n",
    "                cleaned_tags.append(cleaned_tag)\n",
    "\n",
    "    # Write cleaned tags to a new file\n",
    "    output_path = os.path.join(output_folder, 'cleaned_tags.csv')\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['tag_name'])\n",
    "        for tag in sorted(cleaned_tags):  # Sort for consistency\n",
    "            writer.writerow([tag])\n",
    "    return output_path\n",
    "\n",
    "# Step 3: Generate relevant tags using cleaned tags for each file\n",
    "def generate_tags_with_llm(lista_path, cleaned_tags_path, output_folder, model_name):\n",
    "    # Load predefined tags\n",
    "    with open(cleaned_tags_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        predefined_tags = [row['tag_name'].strip() for row in reader if 'tag_name' in row]\n",
    "\n",
    "    max_tags = 50\n",
    "    predefined_tags = predefined_tags[:max_tags]\n",
    "\n",
    "    with open(lista_path, 'r') as f:\n",
    "        file_paths = [line.strip().replace('.mp3', '.txt') for line in f.readlines()]\n",
    "\n",
    "    total_files = len(file_paths)\n",
    "    pbar = tqdm(total=total_files, desc=f\"Generating tags with LLM ({model_name})\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                original_text = f.read()\n",
    "\n",
    "            chunks = split_into_chunks(original_text, chunk_size=100)\n",
    "            all_tags_with_similarity = []\n",
    "            used_tags = set()  # Track tags that have already been used for this file\n",
    "\n",
    "            for chunk in chunks:\n",
    "                # Filter predefined_tags to exclude used tags\n",
    "                available_tags = [tag for tag in predefined_tags if tag not in used_tags]\n",
    "\n",
    "                # If no tags are left, skip further processing for this chunk\n",
    "                if not available_tags:\n",
    "                    break\n",
    "\n",
    "                response = ollama.chat(\n",
    "                    model=model_name,\n",
    "                    messages=[{\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"\"\"\n",
    "                        You are a professional tagger. Your task is to analyze the given text and select exactly 3 relevant tags from the provided list of tags.\n",
    "\n",
    "                        Instructions:\n",
    "                        1. Do not summarize the text.\n",
    "                        2. Only select 3 tags from the provided list.\n",
    "                        3. Do not provide any extra text or explanations, just return the 3 tags.\n",
    "                        4. Separate the tags with commas (without spaces or other punctuation).\n",
    "\n",
    "                        Select 3 tags from the following list:\n",
    "                        {', '.join(available_tags)}\n",
    "\n",
    "                        Text:\n",
    "                        \"{chunk}\"\n",
    "                        \"\"\"\n",
    "                    }]\n",
    "                )\n",
    "\n",
    "                tags = []\n",
    "                if isinstance(response, list):\n",
    "                    for message in response:\n",
    "                        if 'content' in message:\n",
    "                            tags = [tag.strip() for tag in message['content'].strip().split(',')]\n",
    "                            break\n",
    "                elif isinstance(response, dict):\n",
    "                    tags = [tag.strip() for tag in response.get('message', {}).get('content', \"No content available\").strip().split(',')]\n",
    "                else:\n",
    "                    tags = [\"error\", \"generating\", \"tags\"]\n",
    "\n",
    "                # Update used tags to ensure no duplicates are generated for this file\n",
    "                for tag in tags:\n",
    "                    if tag in available_tags:\n",
    "                        used_tags.add(tag)\n",
    "\n",
    "                # Compute cosine similarity for each tag\n",
    "                chunk_embedding = compute_embeddings([chunk])\n",
    "                for tag in tags:\n",
    "                    tag_embedding = compute_embeddings([tag])\n",
    "                    similarity = compute_cosine_similarity(tag_embedding, chunk_embedding)\n",
    "                    all_tags_with_similarity.append((tag, similarity))\n",
    "\n",
    "            unique_tags_with_similarity = list(set(all_tags_with_similarity))\n",
    "\n",
    "            output_csv_path = os.path.join(output_folder, os.path.basename(file_path).replace('.txt', '_tag_final.csv'))\n",
    "            with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(['tag_name', 'cosine_similarity'])\n",
    "                for tag, similarity in sorted(unique_tags_with_similarity, key=lambda x: x[1], reverse=True):\n",
    "                    writer.writerow([tag.strip(), f\"{similarity:.4f}\"])\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a5f9fc-9e21-4be0-8c93-1f3f13436501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: mistral:7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining tags from files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 97/97 [00:00<00:00, 308.07it/s]\n",
      "Generating tags with LLM (mistral:7b):   2%|███                                                                                                                                                   | 2/97 [00:07<05:54,  3.74s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m cleaned_tags_path \u001b[38;5;241m=\u001b[39m clean_tags(combined_tags_path, output_folder)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Step 3: Generate relevant tags using cleaned tags for each file\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m generate_tags_with_llm(lista_path, cleaned_tags_path, output_folder, model_name)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished processing model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 196\u001b[0m, in \u001b[0;36mgenerate_tags_with_llm\u001b[1;34m(lista_path, cleaned_tags_path, output_folder, model_name)\u001b[0m\n\u001b[0;32m    194\u001b[0m chunk_embedding \u001b[38;5;241m=\u001b[39m compute_embeddings([chunk])\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[1;32m--> 196\u001b[0m     tag_embedding \u001b[38;5;241m=\u001b[39m compute_embeddings([tag])\n\u001b[0;32m    197\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m compute_cosine_similarity(tag_embedding, chunk_embedding)\n\u001b[0;32m    198\u001b[0m     all_tags_with_similarity\u001b[38;5;241m.\u001b[39mappend((tag, similarity))\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mcompute_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:976\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    974\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 976\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    977\u001b[0m     embedding_output,\n\u001b[0;32m    978\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    979\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    980\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    981\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m    982\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    983\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    984\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    985\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    986\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    987\u001b[0m )\n\u001b[0;32m    988\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    989\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         output_attentions,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    632\u001b[0m         hidden_states,\n\u001b[0;32m    633\u001b[0m         attention_mask,\n\u001b[0;32m    634\u001b[0m         layer_head_mask,\n\u001b[0;32m    635\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    636\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    637\u001b[0m         past_key_value,\n\u001b[0;32m    638\u001b[0m         output_attentions,\n\u001b[0;32m    639\u001b[0m     )\n\u001b[0;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:562\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    559\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    560\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 562\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    564\u001b[0m )\n\u001b[0;32m    565\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:575\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    574\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 575\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:486\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 486\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    487\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define base folder paths\n",
    "    base_folder = \"voiceapp/output\"\n",
    "    model_name = \"mistral-small\"\n",
    "\n",
    "    # Step 1: Create output folder for the model\n",
    "    output_folder = create_output_folder(model_name)\n",
    "\n",
    "    # Step 2: Combine tags from all *_best_tags.csv files\n",
    "    print(\"Combining tags from all *_best_tags.csv files...\")\n",
    "    combine_tags_path = combine_tags_from_files(base_folder, output_folder)\n",
    "    print(f\"Combined tags saved at: {combine_tags_path}\")\n",
    "\n",
    "    # Step 3: Clean combined tags\n",
    "    print(\"Cleaning combined tags...\")\n",
    "    cleaned_tags_path = clean_tags(combine_tags_path, output_folder)\n",
    "    print(f\"Cleaned tags saved at: {cleaned_tags_path}\")\n",
    "\n",
    "    # Step 4: Generate tags for input files\n",
    "    lista_path = os.path.join(base_folder, \"lista.txt\")  # Ensure this file exists\n",
    "    if os.path.exists(lista_path):\n",
    "        print(\"Generating relevant tags for each file in lista.txt...\")\n",
    "        generate_tags_with_llm(lista_path, cleaned_tags_path, output_folder, model_name)\n",
    "        print(f\"Tag generation completed. Output saved in: {output_folder}\")\n",
    "    else:\n",
    "        print(f\"File 'lista.txt' not found at: {lista_path}. Please create this file with paths to input files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f813d1d-4e8e-478f-82b9-b3d2b63a7513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output folder...\n",
      "Collecting tags from files...\n",
      "Collected 851 tags.\n",
      "Cleaning tags...\n",
      "Cleaned 716 tags.\n",
      "Cleaned tags saved to voiceapp\\output\\combined_tags_folder\\mistral-small_20241221_005553\\cleaned_tags.csv\n",
      "Generating tags with LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tags with LLM (mistral-small):  58%|██████████▍       | 56/97 [08:36<02:55,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - dĹ‚ugi 1.txt not found. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tags with LLM (mistral-small):  73%|█████████████▏    | 71/97 [09:19<01:23,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - los klasy Ĺ›redniej 1.txt not found. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tags with LLM (mistral-small):  75%|█████████████▌    | 73/97 [09:21<00:54,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - mieszkania dla mĹ‚odych 1.txt not found. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tags with LLM (mistral-small):  89%|███████████████▉  | 86/97 [09:55<00:31,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - upadek bankĂłw 1.txt not found. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tags with LLM (mistral-small):  94%|████████████████▉ | 91/97 [10:06<00:14,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - wÄ™giel cz1.txt not found. Skipping.\n",
      "File C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - wÄ™giel po zimie 1.txt not found. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tags with LLM (mistral-small): 100%|██████████████████| 97/97 [10:18<00:00,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\shorts\\CJG - Ĺ‚apĂłwki 1.txt not found. Skipping.\n",
      "Tag generation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import ollama\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Helper to get the current timestamp\n",
    "def get_timestamp():\n",
    "    return time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Function to create a folder based on the model name and timestamp\n",
    "def create_output_folder(model_name):\n",
    "    \"\"\"\n",
    "    Creates a timestamped output folder for a given model name.\n",
    "    Replaces forbidden characters in the folder name.\n",
    "    \"\"\"\n",
    "    # Create the base output and tagging folders\n",
    "    base_output_path = os.path.join(\"voiceapp\", \"output\")\n",
    "    combined_tags_folder_path = os.path.join(base_output_path, \"combined_tags_folder\")\n",
    "    \n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    os.makedirs(combined_tags_folder_path, exist_ok=True)\n",
    "\n",
    "    # Sanitize the model name by replacing forbidden characters with '_'\n",
    "    safe_model_name = re.sub(r'[<>:\"/\\\\|?*]', '_', model_name)\n",
    "\n",
    "    # Generate a timestamp and create the final folder name\n",
    "    timestamp = get_timestamp()\n",
    "    folder_name = f\"{safe_model_name}_{timestamp}\"\n",
    "\n",
    "    # Create the model-specific folder\n",
    "    model_folder_path = os.path.join(combined_tags_folder_path, folder_name)\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    return model_folder_path\n",
    "\n",
    "\n",
    "# Load a pre-trained model for semantic similarity\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "\n",
    "# Function to compute embeddings\n",
    "def compute_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Function to compute cosine similarity between two embeddings\n",
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Function to split text into chunks of approximately 100 words\n",
    "def split_into_chunks(text, chunk_size=100):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Function to recursively search for CSV files containing '_best_tags.csv'\n",
    "def collect_tags_from_files(root_folder):\n",
    "    combined_tags = set()\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('_best_tags.csv'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "                    for row in reader:\n",
    "                        if 'tag_name' in row:\n",
    "                            combined_tags.add(row['tag_name'].strip())\n",
    "    return combined_tags\n",
    "\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Funkcja czyszcząca tagi\n",
    "def clean_tags(tags):\n",
    "    def is_similar(word1, word2, threshold=0.8):\n",
    "        \"\"\"Sprawdza, czy dwa tagi są podobne na podstawie współczynnika podobieństwa.\"\"\"\n",
    "        return SequenceMatcher(None, word1, word2).ratio() > threshold\n",
    "\n",
    "    cleaned_tags = []\n",
    "    \n",
    "    for tag in tags:\n",
    "        # Konwertujemy tag na małe litery\n",
    "        cleaned_tag = tag.strip().lower()\n",
    "        \n",
    "        # Zastępujemy wszystkie znaki specjalne (z wyjątkiem ostatniego znaku) na \"_\"\n",
    "        if len(cleaned_tag) > 1:\n",
    "            cleaned_tag = re.sub(r'[^a-zA-Z0-9_-]', '_', cleaned_tag[:-1]) + cleaned_tag[-1]\n",
    "        else:\n",
    "            cleaned_tag = re.sub(r'[^a-zA-Z0-9_-]', '_', cleaned_tag)\n",
    "\n",
    "        # Sprawdzamy, czy tag jest unikalny i czy spełnia warunki długości\n",
    "        if cleaned_tag not in cleaned_tags and len(cleaned_tag) <= 50 and re.match(\"^[a-zA-Z0-9_-]+$\", cleaned_tag):\n",
    "            if all(not is_similar(cleaned_tag, existing_tag) for existing_tag in cleaned_tags):\n",
    "                cleaned_tags.append(cleaned_tag)\n",
    "    \n",
    "    return cleaned_tags\n",
    "\n",
    "# Function to save cleaned tags to a CSV file\n",
    "def save_cleaned_tags(cleaned_tags, output_folder):\n",
    "    output_path = os.path.join(output_folder, 'cleaned_tags.csv')\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['tag_name'])\n",
    "        for tag in sorted(cleaned_tags):  # Sort for consistency\n",
    "            writer.writerow([tag])\n",
    "    return output_path\n",
    "\n",
    "# Function to generate tags with LLM\n",
    "def generate_tags_with_llm(lista_path, cleaned_tags_path, output_folder, model_name):\n",
    "    # Load predefined tags\n",
    "    with open(cleaned_tags_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        predefined_tags = [row['tag_name'].strip() for row in reader if 'tag_name' in row]\n",
    "\n",
    "    max_tags = 50\n",
    "    predefined_tags = predefined_tags[:max_tags]\n",
    "\n",
    "    # Read file paths from lista.txt and change .mp3 to .txt\n",
    "    with open(lista_path, 'r') as f:\n",
    "        file_paths = [line.strip().replace('.mp3', '.txt') for line in f.readlines()]\n",
    "\n",
    "    total_files = len(file_paths)\n",
    "    pbar = tqdm(total=total_files, desc=f\"Generating tags with LLM ({model_name})\")\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    original_text = f.read()\n",
    "\n",
    "                # Split the text into chunks\n",
    "                chunks = split_into_chunks(original_text, chunk_size=100)\n",
    "                all_tags_with_similarity = []\n",
    "                used_tags = set()\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    available_tags = [tag for tag in predefined_tags if tag not in used_tags]\n",
    "                    if not available_tags:\n",
    "                        break\n",
    "\n",
    "                    response = ollama.chat(\n",
    "                        model=model_name,\n",
    "                        messages=[{\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"\"\"\n",
    "                        You are a professional tagger. Your task is to analyze the given text and select best relevant tags from the provided list of tags.\n",
    "\n",
    "                        Instructions:\n",
    "                        1. Do not summarize the text.\n",
    "                        2. Only select tags from the provided list.\n",
    "                        3. Do not provide any extra text or explanations, just return the best tags.\n",
    "                        4. Separate the tags with commas (without spaces or other punctuation).\n",
    "\n",
    "                        Select 3 tags from the following list:\n",
    "                        {', '.join(available_tags)}\n",
    "\n",
    "                        Text:\n",
    "                        \"{chunk}\"\n",
    "                        \"\"\"\n",
    "                        }]\n",
    "                    )\n",
    "\n",
    "                    tags = response.get('message', {}).get('content', \"No content available\").strip().split(',')\n",
    "                    for tag in tags:\n",
    "                        if tag in available_tags:\n",
    "                            used_tags.add(tag)\n",
    "\n",
    "                    chunk_embedding = compute_embeddings([chunk])\n",
    "                    for tag in tags:\n",
    "                        tag_embedding = compute_embeddings([tag])\n",
    "                        similarity = compute_cosine_similarity(tag_embedding, chunk_embedding)\n",
    "                        all_tags_with_similarity.append((tag, similarity))\n",
    "\n",
    "                unique_tags_with_similarity = list(set(all_tags_with_similarity))\n",
    "                output_csv_path = os.path.join(output_folder, os.path.basename(file_path).replace('.txt', '_tags_final.csv'))\n",
    "                with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['tag_name', 'cosine_similarity'])\n",
    "                    for tag, similarity in sorted(unique_tags_with_similarity, key=lambda x: x[1], reverse=True):\n",
    "                        writer.writerow([tag.strip(), f\"{similarity:.4f}\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"File {file_path} not found. Skipping.\")\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    model_name = \"mistral-small\"  # Model name (Mistral)\n",
    "\n",
    "    # Step 1: Create output folder\n",
    "    print(\"Creating output folder...\")\n",
    "    output_folder = create_output_folder(model_name)\n",
    "\n",
    "    root_folder = \"voiceapp/output/tagging_folder\"  # Folder containing '_best_tags.csv' files\n",
    "    lista_path = \"voiceapp/lista-1.txt\"  # Path to lista.txt\n",
    "\n",
    "    # Step 2: Collect tags from files\n",
    "    print(\"Collecting tags from files...\")\n",
    "    combined_tags = collect_tags_from_files(root_folder)\n",
    "    print(f\"Collected {len(combined_tags)} tags.\")\n",
    "\n",
    "    # Step 3: Clean the tags\n",
    "    print(\"Cleaning tags...\")\n",
    "    cleaned_tags = clean_tags(combined_tags)\n",
    "    print(f\"Cleaned {len(cleaned_tags)} tags.\")\n",
    "\n",
    "    # Step 4: Save cleaned tags to CSV\n",
    "    cleaned_tags_path = save_cleaned_tags(cleaned_tags, output_folder)\n",
    "    print(f\"Cleaned tags saved to {cleaned_tags_path}\")\n",
    "\n",
    "    # Step 5: Generate tags for each transcription\n",
    "    print(\"Generating tags with LLM...\")\n",
    "    generate_tags_with_llm(lista_path, cleaned_tags_path, output_folder, model_name)\n",
    "    print(\"Tag generation completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b2f6b-63d8-46f7-ba58-cf053645b09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
